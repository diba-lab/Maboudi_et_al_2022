{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kevin-maze1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "from scipy.stats import ranksums\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from learned_tuning.learned_tuning import calculate_learned_tuning_PBE_subsets, calculate_place_field_fidelity_of_learned_tuning\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "data_dir = r'/home/kouroshmaboudi/Documents/Learned_tuning_Python/Datasets'\n",
    "sessions = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "# included_session_IDX = [x for x in range(17) if x not in (12, 13)]\n",
    "included_session_IDX = [5]\n",
    "\n",
    "sessions = [sessions[i] for i in included_session_IDX]\n",
    "\n",
    "for session_idx, session_name in enumerate(sessions):\n",
    "\n",
    "    print(session_name)\n",
    "\n",
    "    session_dataset_path = os.path.join(data_dir, session_name)\n",
    "    session_number = included_session_IDX[session_idx]\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    # Load epochs information\n",
    "\n",
    "    filename = f'{session_name}.fileInfo_for_python.mat'\n",
    "    file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "    mat_file = scipy.io.loadmat(file_path)\n",
    "    session_info = mat_file[\"fileInfo\"]\n",
    "\n",
    "    epochs = session_info[\"behavior\"][0][0][0][0][\"time\"]\n",
    "\n",
    "    epoch_names = ['PRE', 'MAZE', 'POST']\n",
    "   \n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Brain state detection results\n",
    "\n",
    "    if 0<=session_number<=4 or 6<=session_number<=10:\n",
    "        filename = f'{session_name}.brainStateDetection_HMMtheta_EMG_SWS_SchmidtTrigger.mat'\n",
    "        file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "        mat_file = scipy.io.loadmat(file_path)\n",
    "        brainStates_bouts_label = mat_file['brainState']['bouts'][0][0][:, :-1]\n",
    "        bouts_start_end = brainStates_bouts_label[:, :-1]/3600\n",
    "        bout_labels = brainStates_bouts_label[:, -1].astype(int)\n",
    "\n",
    "        brainStates_names = []\n",
    "        for i in range(4):\n",
    "            brainStates_names.append(mat_file['brainState']['names'][0][0][i][0][0])\n",
    "    else:\n",
    "        filename = f'{session_name}.fileInfo_for_python.mat'\n",
    "        file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "        mat_file = scipy.io.loadmat(file_path)\n",
    "        brainStates_bouts_label = mat_file['fileInfo']['brainStates'].item()\n",
    "\n",
    "        bouts_start_end = brainStates_bouts_label[:, :-1]/3600\n",
    "        bout_labels = brainStates_bouts_label[:, -1].astype(int)\n",
    "\n",
    "        # swapping the 3s and 4s, because the 3s in Hiro's data are QWAKE while in the other datasets they are active wake(WAKE)\n",
    "        bout_labels_temp = bout_labels.copy()\n",
    "        bout_labels_temp[bout_labels == 3] = 4\n",
    "        bout_labels_temp[bout_labels == 4] = 3\n",
    "        bout_labels = bout_labels_temp[:]\n",
    "        del bout_labels_temp\n",
    "\n",
    "        brainStates_names = ['NREM', 'REM', 'WAKE', 'QWAKE']\n",
    "        \n",
    "\n",
    "\n",
    "    # --- from Bapun detection results to confirm \n",
    "\n",
    "    # filename = f'{session_name}.brainstates.npy'\n",
    "    # file_path = os.path.join(session_dataset_path, filename)\n",
    "    # brain_states_Bapun = np.load(file_path, allow_pickle = True).item()\n",
    "\n",
    "    # bouts_start_bapun = brain_states_Bapun['epochs']['start']/3600\n",
    "    # bouts_end_bapun = brain_states_Bapun['epochs']['stop']/3600\n",
    "\n",
    "    # bout_labels_Bapun = brain_states_Bapun['epochs']['label']\n",
    "\n",
    "    # for i in range(bout_labels_Bapun.shape[0]):\n",
    "    #     if bout_labels_Bapun[i] == 'NREM':\n",
    "    #         bout_labels_Bapun[i] = 1\n",
    "    #     elif bout_labels_Bapun[i] == 'REM':\n",
    "    #         bout_labels_Bapun[i] = 2\n",
    "    #     elif bout_labels_Bapun[i] == 'AW':\n",
    "    #         bout_labels_Bapun[i] = 3\n",
    "    #     elif bout_labels_Bapun[i] == 'QW':\n",
    "    #         bout_labels_Bapun[i] = 4\n",
    "\n",
    "\n",
    "\n",
    "     #----------------------------------------------------------------------------------------------\n",
    "    # Load spike data\n",
    "\n",
    "    filename = f'{session_name}.spikes_for_python.mat'\n",
    "    file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "    mat_file = scipy.io.loadmat(file_path)\n",
    "    spikes_pyr = mat_file[\"spikes_pyr\"]\n",
    "\n",
    "    #### Extracting all place fields from the imported .mat file\n",
    "    spatial_tuning_smoothed = spikes_pyr[\"spatialTuning_smoothed\"]\n",
    "\n",
    "    num_units    = spatial_tuning_smoothed[0].shape[0]\n",
    "    # num_units    = spatial_tuning_smoothed.shape[0] # for RatN only\n",
    "\n",
    "    num_pos_bins = spatial_tuning_smoothed[0][0]['uni'][0][0].size\n",
    "\n",
    "    # print(num_units, num_pos_bins)\n",
    "\n",
    "    spikes = []; # spike data and place field info of each unit\n",
    "\n",
    "    # attributes = list(spikes_pyr.dtype.names) % if we want to work on all variable in the imported .mat data structure\n",
    "    running_directions = {'LR', 'RL', 'uni'}\n",
    "    other_attributes   = {'spike_times', 'shank_id','cluster_id'}\n",
    "\n",
    "    iter = 0\n",
    "    for unit in range(num_units):\n",
    "        \n",
    "        # Create dictionaries for each unit and store the matrices\n",
    "        \n",
    "        unit_spikes = dict()\n",
    "        \n",
    "        unit_spikes['place_fields']  = {}\n",
    "        unit_spikes['peak_pos_bins'] = {}\n",
    "\n",
    "        \n",
    "        for direction in running_directions:\n",
    "            try:\n",
    "                unit_spikes['place_fields'][direction] = spatial_tuning_smoothed[0][unit][direction][0][0].reshape(num_pos_bins) \n",
    "                unit_spikes['peak_pos_bins'][direction] = spikes_pyr['peakPosBin'][0][unit][direction][0][0][0][0]\n",
    "\n",
    "                # unit_spikes['place_fields'][direction] = spatial_tuning_smoothed[unit][0][direction][0][0].reshape(num_pos_bins) \n",
    "                # unit_spikes['peak_pos_bins'][direction] = spikes_pyr['peakPosBin'][unit][0][direction][0][0][0][0]\n",
    "            except ValueError:\n",
    "                if iter == 0:\n",
    "                    print(\"This session has only one running direction\")\n",
    "                iter += 1\n",
    "\n",
    "\n",
    "        unit_spikes['spike_times'] = spikes_pyr['time'][0][unit]\n",
    "        unit_spikes['shank_id']    = spikes_pyr['id'][0][unit][0][0]\n",
    "        unit_spikes['cluster_id']  = spikes_pyr['id'][0][unit][0][1]\n",
    "\n",
    "        # unit_spikes['spike_times'] = spikes_pyr['time'][unit][0] # for RatN and RatS\n",
    "        # unit_spikes['shank_id']    = spikes_pyr['id'][unit][0][0][0]\n",
    "        # unit_spikes['cluster_id']  = spikes_pyr['id'][unit][0][0][1]\n",
    "        \n",
    "            \n",
    "        spikes.append(unit_spikes) \n",
    "\n",
    "    # place fields by pooling spikes across both running directions\n",
    "    place_fields_uni = []\n",
    "    for unit in range(num_units):\n",
    "        place_fields_uni.append(spikes[unit]['place_fields']['uni'])\n",
    "    place_fields_uni = np.array(place_fields_uni)\n",
    "\n",
    "    place_fields_uni[place_fields_uni == 0] = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # Load cluster quality data (L-ratios)\n",
    "    \n",
    "    filename = f'{session_name}.clusterQuality.mat'\n",
    "    file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "    mat_file = scipy.io.loadmat(file_path)\n",
    "\n",
    "    # Access data structure\n",
    "    num_shanks = len(mat_file[\"clusterQuality\"][\"Lratio\"][0])\n",
    "\n",
    "    L_ratios = list()\n",
    "    for shank in range(num_shanks):    \n",
    "        curr_shank_L_ratios = dict()\n",
    "        curr_shank_L_ratios[\"L_ratios\"] = mat_file[\"clusterQuality\"][\"Lratio\"][0][shank]\n",
    "        curr_shank_L_ratios[\"cluster_ids\"] = mat_file[\"clusterQuality\"][\"clus\"][0][shank]\n",
    "        \n",
    "        L_ratios.append(curr_shank_L_ratios)\n",
    "    # L_ratios = []\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    # Population Burst Events (PBEs)\n",
    "\n",
    "    filename = f'{session_name}.PBEInfo_replayScores.mat'\n",
    "    # filename = f'{session_name}.PBEInfo.mat'\n",
    "\n",
    "\n",
    "    file_path = os.path.join(session_dataset_path, filename)\n",
    "    f = h5py.File(file_path, \"r\")\n",
    "\n",
    "    PBEInfo = f['PBEInfo_replayScores']\n",
    "    # PBEInfo = f['PBEInfo_Bayesian']\n",
    "\n",
    "\n",
    "\n",
    "    # Store the population burst events in an object easy to work with in Python \n",
    "    PBEs = []\n",
    "    num_PBEs = PBEInfo[\"fr_1msbin\"].shape[0]\n",
    "    attributes = list(PBEInfo.keys())\n",
    "\n",
    "    # Loop over the PBEs and create a dictionary for each one\n",
    "    num_dots = int(num_PBEs * 0.1)\n",
    "\n",
    "    count = 0\n",
    "    for pbe in range(num_PBEs): #   \n",
    "        # Create a dictionary for the PBE and store the matrices\n",
    "        PBE_dict = {}\n",
    "        for attr in attributes:\n",
    "            ref = PBEInfo[attr][pbe][0]\n",
    "            obj = f[ref] \n",
    "            PBE_dict[attr] = np.array(obj) \n",
    "\n",
    "        PBEs.append(PBE_dict)\n",
    "\n",
    "        if (pbe+1) % num_dots == 1:\n",
    "            count += 1\n",
    "            message = \"Importing PBEs\" + \".\" * count\n",
    "            print(message, end=\"\\r\")\n",
    "\n",
    "    print(\"All PBEs were imported\")\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # Import results of learned tuning calculation in MALAB\n",
    "\n",
    "    # Load .mat file\n",
    "    filename = f'{session_name}.assemblyTunings_allPBEs_Lthresh1e_3.mat'\n",
    "    file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "    mat_file = scipy.io.loadmat(file_path)\n",
    "\n",
    "    active_units = dict()\n",
    "\n",
    "    active_units[\"pre\"] = mat_file[\"activeUnits\"][\"pre\"][0][0]\n",
    "    active_units[\"maze\"] = mat_file[\"activeUnits\"][\"run\"][0][0]\n",
    "    active_units[\"post\"] = mat_file[\"activeUnits\"][\"post\"][0][0]\n",
    "\n",
    "    active_units_epochs_intersect = np.intersect1d(np.intersect1d(active_units[\"pre\"], active_units[\"post\"]), active_units[\"maze\"])\n",
    "    active_units_epochs_intersect = active_units_epochs_intersect - 1 # to make the indices compatible with the python indexing\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------\n",
    "    # Learned tunings during Non-REM versus Quiet Wake ripples\n",
    "\n",
    "\n",
    "    # The frequency of PBEs ocurring during NREM ('N') or quiet wake ('W')\n",
    "\n",
    "    num_PBEs = len(PBEs)\n",
    "    PBEs_peak_time = np.zeros((num_PBEs, 1))\n",
    "    brain_state_first_letter = np.empty((num_PBEs, 1), dtype='str')\n",
    "\n",
    "    for pbe in range(num_PBEs):\n",
    "        PBEs_peak_time[pbe] = PBEs[pbe][\"peakT\"]\n",
    "        brain_state_first_letter[pbe]= chr(np.concatenate(PBEs[pbe][\"brainState\"])[0])\n",
    "\n",
    "\n",
    "    # Calculate learned tunings separately for NREM and QW PBEs and calculate thier PF fidleities\n",
    "    time_win_duration = 900 # 15 minutes\n",
    "    step_duration = 300 # 5 minutes\n",
    "\n",
    "    time_bin_duration=0.02\n",
    "    num_PF_shuffles = 10000\n",
    "    learned_tunings_vs_time_NREM_vs_QW = {}  # create an empty dictionary to store the results\n",
    "    \n",
    "\n",
    "    for epoch_idx, epoch in enumerate(epoch_names):\n",
    "            \n",
    "        epoch_duration = epochs[epoch_idx,1] - epochs[epoch_idx,0]\n",
    "            \n",
    "        # PBEs for the current epoch\n",
    "        if_inside_epoch = (PBEs_peak_time >= epochs[epoch_idx,0]) & (PBEs_peak_time <= epochs[epoch_idx,0]+epoch_duration)\n",
    "\n",
    "\n",
    "        # NREM PBEs\n",
    "        select_IDX_NREM = np.where(np.logical_and(if_inside_epoch, brain_state_first_letter == 'N'))[0]\n",
    "        epoch_NREM_PBEs = np.take(PBEs, select_IDX_NREM)\n",
    "        PBEs_peak_time_NREM = PBEs_peak_time[select_IDX_NREM]\n",
    "        num_PBEs_NREM = len(epoch_NREM_PBEs)\n",
    "\n",
    "        # QW PBEs\n",
    "        select_IDX_QW = np.where(np.logical_and(if_inside_epoch, brain_state_first_letter == 'Q'))[0]\n",
    "        epoch_QW_PBEs = np.take(PBEs, select_IDX_QW)\n",
    "        PBEs_peak_time_QW = PBEs_peak_time[select_IDX_QW]\n",
    "        num_PBEs_QW = len(epoch_QW_PBEs)\n",
    "\n",
    "\n",
    "        # learned tunings across time\n",
    "\n",
    "        num_time_wins = np.floor((epoch_duration-time_win_duration)/step_duration).astype(int)\n",
    "\n",
    "            \n",
    "        if epoch_idx == 0: # if PRE, aligns the last time window with MAZE start and go backward in time for definition of the earlier time windows \n",
    "            win_end_times = epochs[epoch_idx, 1] - np.arange(num_time_wins)*step_duration\n",
    "            win_start_times = win_end_times - time_win_duration\n",
    "        else: # if MAZE or POST, alighn the first time bin with MAZE\n",
    "            win_start_times = epochs[epoch_idx, 0] + np.arange(num_time_wins)*step_duration\n",
    "            win_end_times = win_start_times + time_win_duration\n",
    "        win_centers = win_start_times + time_win_duration/2\n",
    "\n",
    "\n",
    "\n",
    "        # sorting the windows chronologically \n",
    "        sort_indices = np.argsort(win_centers)\n",
    "\n",
    "        win_centers = win_centers[sort_indices]\n",
    "        win_start_times = win_start_times[sort_indices]\n",
    "        win_end_times = win_end_times[sort_indices]\n",
    "\n",
    "\n",
    "        PBEs_indices_per_win_NREM = [None for _ in range(num_time_wins)]\n",
    "        num_PBEs_per_win_NREM = []\n",
    "        PBEs_indices_per_win_QW = [None for _ in range(num_time_wins)]\n",
    "        num_PBEs_per_win_QW = []\n",
    "        \n",
    "        for win in range(num_time_wins):\n",
    "\n",
    "            PBEs_indices_per_win_NREM[win] = np.where((PBEs_peak_time_NREM >= win_start_times[win]) & (PBEs_peak_time_NREM <= win_end_times[win]))[0]\n",
    "            num_PBEs_per_win_NREM.append(len(PBEs_indices_per_win_NREM[win]))\n",
    "\n",
    "            PBEs_indices_per_win_QW[win] = np.where((PBEs_peak_time_QW >= win_start_times[win]) & (PBEs_peak_time_QW <= win_end_times[win]))[0]\n",
    "            num_PBEs_per_win_QW.append(len(PBEs_indices_per_win_QW[win]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # NREM #################\n",
    "\n",
    "        # initialize the objects\n",
    "        learned_tunings_NREM_across_time = np.full((num_units, num_pos_bins, num_time_wins), np.nan)\n",
    "        learned_tuning_place_field_pearson_corr_NREM_across_time = np.full((num_units, num_time_wins), np.nan)\n",
    "        median_LT_PF_pearson_corr_NREM_across_time = [None for _ in range(num_time_wins)]\n",
    "\n",
    "        if len(epoch_NREM_PBEs) > 0: # especially for MAZE period we know that there are usually no PBEs present\n",
    "            \n",
    "            # Calculate the learned tunings\n",
    "            print(f\"calculating learned tunings using NREM PBEs in {epoch}\")\n",
    "            learned_tunings_NREM_across_time = calculate_learned_tuning_PBE_subsets(epoch_NREM_PBEs, spikes, PBEs_indices_per_win_NREM, L_ratios, time_bin_duration)\n",
    "\n",
    "            # Calculate the PF fidelity of the NREM learned tunings\n",
    "            for win in range(num_time_wins):\n",
    "                learned_tuning_place_field_pearson_corr_NREM_across_time[active_units_epochs_intersect, win], median_LT_PF_pearson_corr_NREM_across_time[win] = calculate_place_field_fidelity_of_learned_tuning(\n",
    "                    learned_tunings_NREM_across_time[active_units_epochs_intersect, :, win], \n",
    "                    place_fields_uni[active_units_epochs_intersect, :], \n",
    "                    num_PF_shuffles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # QW ###################\n",
    "\n",
    "        # initialize the objects\n",
    "        learned_tunings_QW_across_time = np.full((num_units, num_pos_bins, num_time_wins), np.nan)\n",
    "        learned_tuning_place_field_pearson_corr_QW_across_time = np.full((num_units, num_time_wins), np.nan)\n",
    "        median_LT_PF_pearson_corr_QW_across_time = [None for _ in range(num_time_wins)]\n",
    "\n",
    "        if len(epoch_QW_PBEs) > 0:\n",
    "\n",
    "            # Calculate the learned tunings\n",
    "            print(f\"calculating learned tunings using QW PBEs in {epoch}\")\n",
    "            learned_tunings_QW_across_time = calculate_learned_tuning_PBE_subsets(epoch_QW_PBEs, spikes, PBEs_indices_per_win_QW, L_ratios, time_bin_duration)\n",
    "            \n",
    "            # Calculate the PF fidelity of the QW learned tunings\n",
    "            for win in range(num_time_wins):\n",
    "                learned_tuning_place_field_pearson_corr_QW_across_time[active_units_epochs_intersect, win], median_LT_PF_pearson_corr_QW_across_time[win] = calculate_place_field_fidelity_of_learned_tuning(\n",
    "                    learned_tunings_QW_across_time[active_units_epochs_intersect, :, win], \n",
    "                    place_fields_uni[active_units_epochs_intersect, :], \n",
    "                    num_PF_shuffles)\n",
    "\n",
    "\n",
    "        # Store the results\n",
    "        learned_tunings_vs_time_NREM_vs_QW[epoch] = {\n",
    "            'learned_tunings_NREM_across_time': learned_tunings_NREM_across_time,\n",
    "            'learned_tuning_place_field_pearson_corr_NREM_across_time': learned_tuning_place_field_pearson_corr_NREM_across_time,\n",
    "            'median_LT_PF_pearson_corr_NREM_across_time': median_LT_PF_pearson_corr_NREM_across_time,\n",
    "            'num_PBEs_per_win_NREM':num_PBEs_per_win_NREM,\n",
    "            'learned_tunings_QW_across_time': learned_tunings_QW_across_time,\n",
    "            'learned_tuning_place_field_pearson_corr_QW_across_time': learned_tuning_place_field_pearson_corr_QW_across_time,\n",
    "            'median_LT_PF_pearson_corr_QW_across_time': median_LT_PF_pearson_corr_QW_across_time,\n",
    "            'num_PBEs_per_win_QW':num_PBEs_per_win_QW,\n",
    "            'win_centers': win_centers\n",
    "        }\n",
    "\n",
    "\n",
    "    filename = f'{session_name}.learned_tunings_vs_time_NREM_vs_QW.npy'\n",
    "    file_path = os.path.join(session_dataset_path, filename)\n",
    "    np.save(file_path, learned_tunings_vs_time_NREM_vs_QW)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the previously calculated and stored results\n",
    "\n",
    "data_dir = r'/home/kouroshmaboudi/Documents/Learned_tuning_Python/Datasets'\n",
    "session_name = 'RatU_Day2NSD_2021-07-24_08-16-38'\n",
    "session_dataset_path = os.path.join(data_dir, session_name)\n",
    "\n",
    "\n",
    "filename = f'{session_name}.fileInfo_for_python.mat'\n",
    "file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "mat_file = scipy.io.loadmat(file_path)\n",
    "session_info = mat_file[\"fileInfo\"]\n",
    "\n",
    "epochs = session_info[\"behavior\"][0][0][0][0][\"time\"]\n",
    "\n",
    "epoch_names = ['PRE', 'MAZE', 'POST']\n",
    "\n",
    "\n",
    "filename = f'{session_name}.brainStateDetection_HMMtheta_EMG_SWS_SchmidtTrigger.mat'\n",
    "file_path = os.path.join(session_dataset_path, filename)\n",
    "\n",
    "mat_file = scipy.io.loadmat(file_path)\n",
    "brainStates_bouts_label = mat_file['brainState']['bouts'][0][0][:, :-1]\n",
    "bouts_start_end = brainStates_bouts_label[:, :-1]/3600\n",
    "bout_labels = brainStates_bouts_label[:, -1].astype(int)\n",
    "\n",
    "brainStates_names = []\n",
    "for i in range(4):\n",
    "    brainStates_names.append(mat_file['brainState']['names'][0][0][i][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learned tunings based on 4+ hours data\n",
    "\n",
    "session_dataset_path = os.path.join(data_dir, session_name)\n",
    "\n",
    "filename = f'{session_name}.learned_tunings_vs_time_NREM_vs_QW.npy'\n",
    "file_path = os.path.join(session_dataset_path, filename)\n",
    "learned_tunings_vs_time_NREM_vs_QW = np.load(file_path, allow_pickle = True).item()\n",
    "\n",
    "filename = f'{session_name}.learned_tunings_NREM_vs_QW.npy'\n",
    "file_path = os.path.join(session_dataset_path, filename)\n",
    "learned_tunings_NREM_vs_QW = np.load(file_path, allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the cell above but showing the NREM and QW learned tunings of the neurons next to each other\n",
    "\n",
    "def max_normalize_columns(matrix):\n",
    "    \"\"\"\n",
    "    Takes a matrix as input and returns a new matrix where each column sums up to one (a probability distribution)\n",
    "    \"\"\"\n",
    "    # calculate the sum of each column\n",
    "    col_max = np.max(matrix, axis=0)\n",
    "    \n",
    "    # divide each element in a column by its sum\n",
    "    normalized_matrix = matrix / col_max[np.newaxis, :]\n",
    "    \n",
    "    return normalized_matrix\n",
    "\n",
    "\n",
    "# There is a mismatch between the unit indices in the bulk (4 hours version) and time-resolved (time windows) learned tunings. In the bulk version the unit identities correspond to active units\n",
    "# while in the time-window version, all units are present. To resolve the mismatch we need to calcualte the active units again.\n",
    "\n",
    "filename = f'{session_name}.assemblyTunings_allPBEs_Lthresh1e_3.mat'\n",
    "file_path = os.path.join(session_dataset_path, filename)\n",
    "mat_file = scipy.io.loadmat(file_path)\n",
    "\n",
    "active_units = dict()\n",
    "active_units[\"pre\"] = mat_file[\"activeUnits\"][\"pre\"][0][0]\n",
    "active_units[\"maze\"] = mat_file[\"activeUnits\"][\"run\"][0][0]\n",
    "active_units[\"post\"] = mat_file[\"activeUnits\"][\"post\"][0][0]\n",
    "\n",
    "active_units_epochs_intersect = np.intersect1d(np.intersect1d(active_units[\"pre\"], active_units[\"post\"]), active_units[\"maze\"])\n",
    "active_units_epochs_intersect = active_units_epochs_intersect - 1 # to make the indices compatible with the python indexing\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the learned across time \n",
    "\n",
    "# Choosing units with maximum PF fidelity\n",
    "learned_tuning_place_field_pearson_corr_NREM = learned_tunings_NREM_vs_QW['POST']['learned_tuning_place_field_pearson_corr_NREM']\n",
    "\n",
    "unit_sort_IDX = np.argsort(learned_tuning_place_field_pearson_corr_NREM)\n",
    "units_t_plot = unit_sort_IDX[-15:-1]\n",
    "\n",
    "\n",
    "learned_tunings_vs_time_active_units = dict()\n",
    "for epoch in epoch_names:\n",
    "    learned_tunings_vs_time_active_units[epoch] = dict()\n",
    "    learned_tunings_vs_time_active_units[epoch]['NREM'] = learned_tunings_vs_time_NREM_vs_QW[epoch]['learned_tunings_NREM_across_time'][active_units_epochs_intersect]\n",
    "    learned_tunings_vs_time_active_units[epoch]['QW'] = learned_tunings_vs_time_NREM_vs_QW[epoch]['learned_tunings_QW_across_time'][active_units_epochs_intersect]\n",
    "\n",
    "num_pos_bins = learned_tunings_vs_time_active_units[epoch]['NREM'].shape[0]\n",
    "\n",
    "\n",
    "# num_units, num_pos_bins, _ = learned_tunings_vs_time_NREM_vs_QW_active_units_POST.shape\n",
    "win_centers_POST = learned_tunings_vs_time_NREM_vs_QW['POST']['win_centers']\n",
    "win_centers_POST = win_centers_POST/3600\n",
    "\n",
    "win_centers_PRE = learned_tunings_vs_time_NREM_vs_QW['PRE']['win_centers']\n",
    "win_centers_PRE = win_centers_PRE/3600\n",
    "\n",
    "\n",
    "# median_LT_PF_corr = []\n",
    "# for win in range(win_centers.shape[0]):\n",
    "#     median_LT_PF_corr.append(learned_tunings_vs_time_NREM_vs_QW['POST']['median_LT_PF_pearson_corr_NREM_across_time'][win][\"data\"])\n",
    "\n",
    "fig, axes = plt.subplots(units_t_plot.shape[0]+2,1, sharex=True, gridspec_kw = {'height_ratios':[.1, .6]+[1]*units_t_plot.shape[0], 'width_ratios': [1]}) \n",
    "fig.set_size_inches(5,12) \n",
    "plt.style.context('dark_background')\n",
    "\n",
    "epochs_hours = epochs/3600\n",
    "\n",
    "# plot epoch indicator bars\n",
    "colors=['blue','black','red']\n",
    "for i, epoch in enumerate(epoch_names):\n",
    "    start,end = epochs_hours[i]\n",
    "    axes[0].axvspan(start, end, alpha=0.8, color=colors[i])\n",
    "    axes[0].text(np.mean(epochs_hours[i]), 1.3, epoch, fontsize=8, ha='center', color=colors[i], alpha=0.8)\n",
    "axes[0].axis('off')\n",
    "\n",
    "\n",
    "# plot brain state indicator bar\n",
    "colors = sns.color_palette('Paired', n_colors=4)\n",
    "colors = [colors[0], colors[1], colors[3], colors[2]]\n",
    "\n",
    "for bout_idx in range(bout_labels.shape[0]):\n",
    "    axes[1].barh(y = bout_labels[bout_idx], left=bouts_start_end[bout_idx, 0], width = bouts_start_end[bout_idx,1]-bouts_start_end[bout_idx,0], height=1, color=colors[bout_labels[bout_idx]-1]) #\n",
    "# axes[1].axis('off')\n",
    "axes[1].set_yticks([1,2,3,4],  brainStates_names)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    "\n",
    "# # plot brain states based on Bapun's calculations\n",
    "# for bout_idx in range(bout_labels_Bapun.shape[0]):\n",
    "#     axes[2].barh(y = bout_labels_Bapun[bout_idx], left=bouts_start_bapun[bout_idx], width = bouts_end_bapun[bout_idx]-bouts_start_bapun[bout_idx], height=1, color=colors[bout_labels_Bapun[bout_idx]-1]) #\n",
    "# # axes[1].axis('off')\n",
    "# axes[2].set_yticks([1,2,3,4],  brainStates_names)\n",
    "# axes[2].tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    " \n",
    "\n",
    "# plot the learned tunings for the example units\n",
    "start_idx = 2\n",
    "for i, curr_unit in enumerate(units_t_plot):\n",
    "\n",
    "    for j, epoch in enumerate(epoch_names):\n",
    "\n",
    "        NREM_LT = learned_tunings_vs_time_active_units[epoch]['NREM'][curr_unit, :, :]\n",
    "        NREM_LT = max_normalize_columns(NREM_LT)\n",
    "\n",
    "        QW_LT = learned_tunings_vs_time_active_units[epoch]['QW'][curr_unit, :, :]\n",
    "        QW_LT = max_normalize_columns(QW_LT)\n",
    "\n",
    "        LT = np.concatenate([QW_LT, NREM_LT], axis=0) # concatenate the NREM and QW learned tunings along position dimension in order to plot them together\n",
    "\n",
    "        axes[i+start_idx].imshow(LT, extent = [epochs_hours[j, 0], epochs_hours[j, 1], 0, num_pos_bins*2], interpolation='nearest', origin = 'lower', cmap='viridis', label=epoch) \n",
    "\n",
    "    axes[i+start_idx].set_xlim([win_centers_PRE[0], win_centers_POST[-1]])\n",
    "    axes[i+start_idx].set_title(f\"unit {curr_unit}\", loc = 'left', fontsize= 8)\n",
    "    axes[i+start_idx].set_aspect(\"auto\")\n",
    "    axes[i+start_idx].tick_params(axis='both', which='major', labelsize=6)\n",
    "    axes[i+start_idx].axhline(y = num_pos_bins, color='white', linestyle='--', linewidth=0.5)\n",
    "    axes[i+start_idx].axvline(x = epochs_hours[1, 0], color='gray', linestyle='-', linewidth=0.5)\n",
    "    axes[i+start_idx].axvline(x = epochs_hours[1, 1], color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    if i == 0:\n",
    "        axes[i+start_idx].text(-1.5, 0.4*num_pos_bins, 'QW:', fontsize=8, ha = 'left', color='black', alpha = 0.8)\n",
    "        axes[i+start_idx].text(-1.5, 1.4*num_pos_bins, 'NREM:', fontsize=8, ha = 'left', color='black', alpha = 0.8)\n",
    "    \n",
    "    axes[i+start_idx].set_yticks([0, num_pos_bins, num_pos_bins*2], [0, '0\\n1', 1])\n",
    "    axes[i+start_idx].set_ylabel('position', fontsize=6)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "# plt.tight_layout()\n",
    "\n",
    "filename = f'{session_name}.QW_NREM_learned_tuning_vs_time.svg'\n",
    "file_path = os.path.join(session_dataset_path, filename)\n",
    "plt.savefig(file_path, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_names = ['PRE', 'MAZE', 'POST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
